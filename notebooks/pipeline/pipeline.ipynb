{"cells":[{"cell_type":"markdown","source":["# Pipeline\nThis notebook defines a sample data processing pipeline as a set of classes. To execute the pipeline, use a **driver** notebook and to test the pipeline use a **tests** notebook. This promotes good coding practices around code structure and code coverage while still working within a Notebook environment."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import DataFrame, SparkSession\nfrom pyspark.sql.functions import expr, col\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## File Access\nThis is an abstract class that forms the interface to read/write DataFrames used by the pipeline. This allows control of the input/output file systems and files for unit testing and allows tests to execute on totally separate data in all environments."],"metadata":{}},{"cell_type":"code","source":["class FileAccess:\n  def __init__(self, spark: SparkSession, dbutils) -> None:\n    self.spark = spark\n    self.dbutils = dbutils\n  def read(self, path: str) -> DataFrame:\n    pass\n  def write(self, path: str, df: DataFrame) -> None:\n    pass"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## DBFS File Access\nThis class is a concrete implementation of the FileAccess class and is provided for the production pipeline. Data is read from and written to DBFS."],"metadata":{}},{"cell_type":"code","source":["class DbfsFileAccess(FileAccess):\n  def read(self, path: str) -> DataFrame:\n    schema=StructType([\n      StructField('id', IntegerType(), False), \n      StructField('name', StringType(), False), \n      StructField('value', StringType(), False)\n    ])\n    return spark.read.format('csv').load(f'dbfs:/tmp/pipeline/{path}', schema=schema, header=True)\n  def write(self, path: str, df: DataFrame):\n    df.write.format('delta').mode('overwrite').save(f'dbfs:/tmp/pipeline/{path}')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Data Pipeline\nThe following class implements the data pipeline"],"metadata":{}},{"cell_type":"code","source":["class Pipeline:\n  def __init__(self, spark: SparkSession, files: FileAccess) -> None:\n    self.spark = spark\n    self.files = files  \n  \n  def _transform(self, df: DataFrame) -> DataFrame:\n    df = df.select(col('id'), expr('UPPER(name) AS name'), col('value'), expr('CAST(1 AS TINYINT) AS processed'), expr('CURRENT_TIMESTAMP() AS processed_time'))\n    return df\n  \n  def run(self) -> None:\n    df = self.files.read('raw.csv')\n    df = self._transform(df)\n    self.files.write('refined', df)"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"pipeline","notebookId":3666776045961402},"nbformat":4,"nbformat_minor":0}
