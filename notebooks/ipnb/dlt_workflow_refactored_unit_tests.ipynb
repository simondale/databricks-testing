{"cells":[{"cell_type":"code","source":["%run ./dlt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22748a1f-cd4c-402e-9dfd-911fde2fdcc5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./dlt_workflow_refactored"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3fc0f1c-bf9c-411e-9f53-7c63a73cd604"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nimport unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9d0c43c-966a-4430-a873-bf0dca4cfa2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nimport datetime\n\n\ntimestamp = datetime.datetime.fromisoformat(\"2000-01-01T00:00:00\")\n\n\ndef timestamp_provider():\n  return lit(timestamp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"375ba02a-d7c6-4030-8308-4e44ddaa384b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import when, col\nfrom pyspark.sql import Row\n\n\nclass FunctionUnitTests(unittest.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    container.register(\n      timestamp_provider=timestamp_provider\n    )\n  \n  def test_add_ingest_columns(self):\n    df = spark.range(1)\n    df = df.transform(container.add_ingest_columns)\n    result = df.collect()\n    self.assertEqual(1, len(result), \"Only one record expected\")\n    self.assertIn(\"ingest_timestamp\", df.columns, \"Ingest timestamp column not present\")\n    self.assertIn(\"ingest_source\", df.columns, \"Ingest source column not present\")\n    self.assertEqual(url.split(\"/\")[-1], result[0].ingest_source, \"Ingest source not correct\")\n    self.assertEqual(timestamp, result[0].ingest_timestamp, \"Ingest timestamp not correct\")\n    \n  def test_add_processed_timestamp(self):\n    df = spark.range(1)\n    df = df.transform(container.add_processed_timestamp)\n    result = df.collect()\n    self.assertEqual(1, len(result), \"Only one record expected\")\n    self.assertIn(\"processed_timestamp\", df.columns, \"Processed timestamp column not present\")\n    self.assertEqual(timestamp, result[0].processed_timestamp, \"Processed timestamp not correct\")\n    \n  def test_add_null_index_array(self):\n    df = spark.createDataFrame([\n      Row(id=1, test_null=None),\n      Row(id=2, test_null=1)\n    ])\n    df = df.transform(container.add_null_index_array)\n    result = df.collect()\n    self.assertEqual(2, len(result), \"Two records are expected\")    \n    self.assertIn(\"nulls\", df.columns, \"Nulls column not present\")\n    self.assertIsNone(result[0].test_null, \"First record should contain null\")\n    self.assertIsNotNone(result[1].test_null, \"Second record should not contain null\")\n    self.assertIn(1, result[0].nulls, \"Nulls array should include 1\")\n    self.assertIsNot(result[1].nulls, \"Nulls array should be empty\")\n    \n  def test_filter_null_index_empty(self):\n    df = spark.createDataFrame([\n      Row(id=1, test_null=None, nulls=[1]),\n      Row(id=2, test_null=1, nulls=[])\n    ])\n    df = df.transform(container.filter_null_index_empty)\n    result = df.collect()\n    self.assertEqual(1, len(result), \"One record is expected\")\n    self.assertNotIn(\"nulls\", df.columns, \"Nulls column not present\")\n    \n  def test_filter_null_index_not_empty(self):\n    df = spark.createDataFrame([\n      Row(id=1, test_null=None, nulls=[1]),\n      Row(id=2, test_null=1, nulls=[])\n    ])\n    df = df.transform(container.filter_null_index_not_empty)\n    result = df.collect()\n    self.assertEqual(1, len(result), \"One record is expected\")\n    self.assertIn(\"nulls\", df.columns, \"Nulls column not present\")\n    \n  def test_agg_count_by_country(self):\n    df = spark.createDataFrame([\n      Row(country=\"Country0\"),\n      Row(country=\"Country1\"),\n      Row(country=\"Country0\")\n    ])\n    df = df.transform(container.agg_count_by_country)\n    result = df.collect()\n    self.assertEqual(2, len(result), \"Two records expected\")\n    self.assertIn(\"country\", df.columns, \"Country column not present\")\n    self.assertIn(\"count\", df.columns, \"Count column not present\")\n    d = {r[0]: r[1] for r in result}\n    self.assertEqual(2, d.get(\"Country0\", -1), \"Country0 count should be 2\")\n    self.assertEqual(1, d.get(\"Country1\", -1), \"Country1 count should be 1\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65c141e4-f01f-4d6a-ae5b-9a801c786a26"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dlt_workflow_refactored_unit_tests","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2584907271439832}},"nbformat":4,"nbformat_minor":0}
