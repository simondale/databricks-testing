# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

stages:

- stage: build
  displayName: Build Databricks Artefacts
  pool:
    vmImage: 'ubuntu-latest'
  jobs:
  - job:
    displayName: Create Build Artefact
    steps:
      - checkout: self
      - task: CopyFiles@2
        displayName: Copy Assets to Staging Folder
        inputs:
          Contents: |
            scripts/**
            notebooks/**
          TargetFolder: $(Build.ArtifactStagingDirectory)
      - task: PublishBuildArtifacts@1
        displayName: Publish Artefacts
        inputs:
          PathtoPublish: $(Build.ArtifactStagingDirectory)
          ArtifactName: drop
          publishLocation: Container

- stage: Development
  dependsOn: Build
  variables:
    - group: databricks-dev
  jobs: 
  - deployment: deploy
    environment: dev
    pool:
      vmImage: 'ubuntu-latest'
    workspace:
      clean: all
    strategy:
      runOnce:
        deploy:
          steps:
          - download: current
            artifact: drop
          - task: UsePythonVersion@0
            displayName: Set Python version
            inputs:
              versionSpec: 3.8
              addToPath: true
              architecture: x64
          - bash: |
              python3 -m venv venv
              source venv/bin/activate
              pip install wheel
              pip install databricks-cli --prefer-binary
            displayName: Setup Python environment
            workingDirectory: $(Pipeline.Workspace)/drop
          - task: AzureCLI@2
            displayName: Create access token
            inputs:
              azureSubscription: DatabricksCICD
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                access_token=$(az account get-access-token --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d --query accessToken -otsv)
                echo "##vso[task.setvariable variable=DatabricksToken]$access_token"
              workingDirectory: $(Pipeline.Workspace)/drop
          - bash: |
              source venv/bin/activate
              databricks workspace mkdirs /DatabricksTesting
              databricks workspace import_dir --overwrite $(Pipeline.Workspace)/drop/notebooks/source /DatabricksTesting
            displayName: Upload notebooks
            workingDirectory: $(Pipeline.Workspace)/drop
            env:
              DATABRICKS_HOST: $(DatabricksHost)
              DATABRICKS_TOKEN: $(DatabricksToken)
          - bash: |
              source venv/bin/activate
              body=$(printf '{
                "run_name": "Unit tests",
                "new_cluster": {
                    "spark_version": "8.3.x-scala2.12",
                    "node_type_id": "Standard_F4s",
                    "num_workers": 1
                },
                "notebook_task": {
                    "notebook_path": "/DatabricksTesting/source"
                }
              }')
              echo $body
              run_id=$(curl -X POST -H "Authorization: Bearer $DATABRICKS_TOKEN" -d @<(echo $body) $DATABRICKS_HOST/api/2.0/jobs/runs/submit | jq -r .run_id)
              echo $run_id
              while true; do
                state=$(curl -H "Authorization: Bearer $DATABRICKS_TOKEN" $DATABRICKS_HOST/api/2.0/jobs/runs/get?run_id=$run_id | jq -r state.life_cycle_state)
                echo $state
                if [ "$state" == "TERMINATED" ]; then
                  break
                fi
                if [ "$state" == "SKIPPED" ]; then
                  echo "SKIPPED"
                  exit 1
                fi
                if [ "$state" == "INTERNAL_ERROR" ]; then
                  echo "INTERNAL_ERROR"
                  exit 1
                fi
                sleep 5
              done
              curl --netrc -H "Authorization: Bearer $DATABRICKS_TOKEN" $DATABRICKS_HOST/api/2.0/jobs/runs/get-output?run_id=$run_id | jq -r .notebook_output.result > TEST-Databricks.xml
            displayName: Execute tests
            workingDirectory: $(Pipeline.Workspace)/drop
            env:
              DATABRICKS_HOST: $(DatabricksHost)
              DATABRICKS_TOKEN: $(DatabricksToken)
          - task: PublishTestResults@2
            inputs:
              testResultsFormat: JUnit
              testResultsFiles: '**/Test-*.xml'
              searchFolder: $(Pipeline.Workspace)/drop
              mergeTestResults: true
              failTaskOnFailedTests: true
              testRunTitle: Databricks Tests
            displayName: Publish test results
            condition: always()
           
