# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- master

stages:

- stage: build
  displayName: Build Databricks Artefacts
  pool:
    vmImage: 'ubuntu-latest'
  jobs:
  - job:
    displayName: Create Build Artefact
    steps:
      - checkout: self
      - task: CopyFiles@2
        displayName: Copy Assets to Staging Folder
        inputs:
          Contents: |
            scripts/**
            notebooks/**
          TargetFolder: $(Build.ArtifactStagingDirectory)
      - task: PublishBuildArtifacts@1
        displayName: Publish Artefacts
        inputs:
          PathtoPublish: $(Build.ArtifactStagingDirectory)
          ArtifactName: drop
          publishLocation: Container

- stage: Development
  dependsOn: Build
  variables:
    - group: dev
  jobs: 
  - deployment: deploy
    environment: dev
    pool:
      vmImage: 'ubuntu-latest'
    workspace:
      clean: all
    strategy:
      runOnce:
        deploy:
          steps:
          - download: current
            artifact: drop
          - task: AzureCLI@2
            inputs:
              azureSubscription: 'Visual Studio Enterprise(abb0e29f-b7c0-4171-a437-d4df920360ab)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                DATABRICKS_URL="https://"$(az resource show --id $DATABRICKS_ID --query properties.workspaceUrl -otsv)
                TENANT_ID=$(az account show --query tenantId -otsv)
                GLOBAL_DATABRICKS_APPID=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d
                AZURE_MANAGEMENT=https://management.azure.com
                aztoken=$(az account get-access-token --resource $AZURE_MANAGEMENT --query accessToken -otsv)
                az rest --method POST --resource $GLOBAL_DATABRICKS_APPID --uri $DATABRICKS_URL/api/2.0/workspace/mkdirs \
                  --headers X-Databricks-Azure-SP-Management-Token="$aztoken" X-Databricks-Azure-Workspace-Resource-Id="$DATABRICKS_ID" \
                  --body '{"path":"/Tests/pipeline"}'

                az rest --method GET --resource $GLOBAL_DATABRICKS_APPID --uri $DATABRICKS_URL/api/2.0/preview/scim/v2/Me \
                  --headers X-Databricks-Azure-SP-Management-Token="$aztoken" X-Databricks-Azure-Workspace-Resource-Id="$DATABRICKS_ID"
                
                az rest --method GET --resource $GLOBAL_DATABRICKS_APPID --uri $DATABRICKS_URL/api/2.0/preview/scim/v2/ServicePrincipals \
                  --headers X-Databricks-Azure-SP-Management-Token="$aztoken" X-Databricks-Azure-Workspace-Resource-Id="$DATABRICKS_ID"

                az account get-access-token
              addSpnToEnvironment: true
            env:
              DATABRICKS_ID: $(DatabricksId)

#          - task: AzureCLI@2
#            displayName: Deploy Databricks Notebooks
#            inputs:
#              azureSubscription: Visual Studio Enterprise(abb0e29f-b7c0-4171-a437-d4df920360ab)
#              scriptType: bash
#              scriptLocation: scriptPath
#              scriptPath: $(Pipeline.Workspace)/drop/scripts/databricks.sh
#              workingDirectory: $(Pipeline.Workspace)/drop/
#            env:
#              DATABRICKS_ID: $(DatabricksId)
#          - task: PublishTestResults@2
#            inputs:
#              testResultsFormat: JUnit
#              testResultsFiles: '**/TEST-*.xml'
#              searchFolder: $(Pipeline.Workspace)/drop/
#              mergeTestResults: true
#              failTaskOnFailedTests: true
#              testRunTitle: Pipeline Tests
            